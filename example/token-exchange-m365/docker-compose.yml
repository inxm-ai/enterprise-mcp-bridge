services:

  jaeger:
    image: cr.jaegertracing.io/jaegertracing/jaeger:latest
    container_name: ops.jaeger
    ports:
      - "16686:16686"
      - "14250:14250"
      - "4317:4317"
      - "4318:4318"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - COLLECTOR_OTLP_HTTP_ENABLED=true
    networks:
      - private
    command: "--set extensions.jaeger_query.base_path=/ops/jaeger"

  keycloak:
    image: quay.io/keycloak/keycloak:25.0.2
    container_name: auth.keycloak
    command: >
      start-dev --import-realm --hostname="auth.inxm.local" --https-port=443 --proxy=edge --hostname-strict=false --hostname-strict-https=false --hostname-strict-backchannel=false
    environment:
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin}
      KC_FEATURES: token-exchange,hostname
      KEYCLOAK_IMPORT: /opt/keycloak/data/import/realm-inxm.json
    volumes:
      - ./keycloak/realm-export:/opt/keycloak/data/import
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "/bin/sh -c 'exec 3<> /dev/tcp/localhost/8080 || exit 1'"]
      interval: 5s
      timeout: 5s
      start_period: 10s
      retries: 30
    networks:
      - private

  oauth2-proxy:
    container_name: auth.oauth2-proxy
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0
    environment:
      OAUTH2_PROXY_LOG_LEVEL: debug
      OAUTH2_PROXY_PROVIDER: "keycloak-oidc"
      OAUTH2_PROXY_CLIENT_ID: "frontend-client"
      OAUTH2_PROXY_CLIENT_SECRET: "this-is-not-a-secret"
      OAUTH2_PROXY_COOKIE_SECRET: "1234567890abcdef"
      OAUTH2_PROXY_COOKIE_SECURE: "false"
      OAUTH2_PROXY_REDIRECT_URL: "https://inxm.local/oauth2/callback"
      OAUTH2_PROXY_UPSTREAMS: "http://app-nginx:3099/"
      OAUTH2_PROXY_EMAIL_DOMAINS: "*"
      OAUTH2_PROXY_OIDC_ISSUER_URL: "https://auth.inxm.local/realms/inxm"
      OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
      OAUTH2_PROXY_REDIS_CONNECTION_URL: "redis://redis:6379"
      OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
      OAUTH2_PROXY_PASS_USER_HEADERS: "true"
      OAUTH2_PROXY_PASS_AUTHORIZATION_HEADER: "true"
      OAUTH2_PROXY_PASS_BASIC_AUTH: "false"
      OAUTH2_PROXY_COOKIE_REFRESH: "5m"
      OAUTH2_PROXY_COOKIE_EXPIRE: "60m"
      OAUTH2_PROXY_SSL_INSECURE_SKIP_VERIFY: "true"
      OAUTH2_PROXY_HTTP_ADDRESS: "0.0.0.0:3098"
      OAUTH2_PROXY_UPSTREAM_TIMEOUT: "30s"
      OAUTH2_PROXY_PASS_ACCESS_TOKEN: "true"
      OAUTH2_PROXY_SET_AUTHORIZATION_HEADER: "true"
      OAUTH2_PROXY_SESSION_STORE_TYPE: "redis"
      OAUTH2_PROXY_OIDC_EXTRA_PARAMS: "kc_idp_hint=ms365"
    ports:
      - "3098:3098"
    depends_on:
      keycloak:
        condition: service_healthy
      app-nginx:
        condition: service_healthy
      ingress:
        condition: service_healthy
      redis:
        condition: service_started

    networks:
      - private

  app-nginx:
    container_name: app.ingress
    build:
      context: ./nginx
      dockerfile: Dockerfile.nginx-with-otel
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "3099:3099"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3099/ || exit 1"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
    depends_on:
      - app-frontend
      - app-mcp-rest
    environment:
      - OAI_HOST=${OAI_HOST}
      - OAI_BASE_URL=${OAI_BASE_URL}
      - OAI_API_TOKEN=${OAI_API_TOKEN}
      - OAI_MODEL_NAME=${OAI_MODEL_NAME}
    networks:
      - private

  app-frontend:
    container_name: app.frontend
    image: node:22-alpine
    working_dir: /usr/src/app
    command: sh -c "npx --yes http-server -p 3001 ."
    volumes:
      - ./web:/usr/src/app:ro
    depends_on:
      - app-mcp-rest
    ports:
      - "3001:3001"
    labels:
      - "oauth2-proxy.enable=true"
    networks:
      - private
  
  app-mcp-rest:
    container_name: app.mcp-m365-rest-server
    image: ghcr.io/inxm-ai/enterprise-mcp-bridge:latest
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      SERVICE_NAME: mcp-m365-rest-server
      MCP_SERVER_COMMAND: npx -y @softeria/ms-365-mcp-server --org-mode
      OAUTH_ENV: MS365_MCP_OAUTH_TOKEN
      AUTH_PROVIDER: keycloak
      AUTH_ALLOW_UNSAFE_CERT: true
      AUTH_BASE_URL: https://auth.inxm.local
      KEYCLOAK_REALM: inxm
      KEYCLOAK_PROVIDER_ALIAS: ms365
      MCP_BASE_PATH: /api/mcp/m365
      EXCLUDE_TOOLS: login,logout,verify-login
      INCLUDE_TOOLS: "*mail*"
      OTLP_ENDPOINT: http://jaeger:4317
      TGI_URL: ${OAI_BASE_URL}
      UVICORN_LOG_LEVEL: debug
      SYSTEM_DEFINED_PROMPTS: >
        [
          {
            "name": "system",
            "title": "Specialized M365 Agent",
            "description": "Acts as a specialized Microsoft 365 agent. Always try to reply with tool_calls unless delivering the final response.",
            "arguments": [],
            "template": {
              "role": "system",
              "content": "You are a highly specialized Microsoft 365 agent. Your primary function is to respond to user requests by identifying and executing the correct Microsoft Graph API tool calls.\n\nYour workflow is strictly as follows:\n1.  **Analyze the user's request.** Determine the user's intent and identify which of your available tools are required.\n2.  **Generate a tool call.** Based on your analysis, construct the appropriate tool call. All values must adhere strictly to the Microsoft Graph API specifications.\n3.  **Strictly adhere to these rules:**\n    * **Prioritize tool calls:** Always respond with a tool call unless you have sufficient information to provide a final, complete answer to the user.\n    * **Use simple requests:** Only provide the minimum required parameters for a tool call. Do not guess or add unnecessary values.\n    * **Correct data types:** If a parameter requires a boolean, use `true` or `false`, not a string. If a parameter requires a number, use a number, not a string.\n    * **If you are failing to call the tool successfully,** review the error message and adjust your request accordingly. Try to remove failing parameters to simplify the request, and expand them in a second pass if they are successful and you know more.\n    * **Final response:** Only provide a final human-readable response when the user's request is fully resolved and no further tool calls are needed.\n\nIf a tool call fails, you will receive an error message. Use this information to correct the tool call in your next turn. Do not generate conversational text or explanations during the tool-calling process; the only exceptions are in a final, complete response or if you are specifically asked to \"think.\"\n\n\n\nExample of a simple, correct tool call:\n{\"name\": \"get-user-profile\", \"parameters\": {\"userId\": \"adeleV@M365x123456.onmicrosoft.com\"}}\n"
            }
          }
        ]
    depends_on:
      keycloak:
        condition: service_healthy
      jaeger:
        condition: service_started
    ports:
      - "8000:8000"
    labels:
      - "oauth2-proxy.enable=true"
    networks:
      - private
      - default

  ingress:
    container_name: ingress
    image: nginx:latest
    volumes:
      - ./nginx/ingress-nginx.conf:/etc/nginx/nginx.conf:ro
      - ./dev-local-certs/auth.inxm.local.crt:/etc/nginx/certs/auth.inxm.local.crt:ro
      - ./dev-local-certs/auth.inxm.local.key:/etc/nginx/certs/auth.inxm.local.key:ro
      - ./dev-local-certs/inxm.local.crt:/etc/nginx/certs/inxm.local.crt:ro
      - ./dev-local-certs/inxm.local.key:/etc/nginx/certs/inxm.local.key:ro
    ports:
      - "443:443"
    depends_on:
      keycloak:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -k --resolve auth.inxm.local:443:127.0.0.1 https://auth.inxm.local:443/realms/inxm || exit 1"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
    networks:
      default: {}
      private:
        aliases:
          - inxm.local
          - auth.inxm.local

  redis:
    container_name: auth.redis
    image: redis:latest
    ports:
      - "6379:6379"
    networks:
      - private

  dummy-llm:
    container_name: dummy.llm
    build:
      context: ./dummy-llm
      dockerfile: Dockerfile
    ports:
      - "8765:8765"
    networks:
      - private

  grafana:
    container_name: ops.grafana
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SERVER_ROOT_URL=https://inxm.local/ops/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      prometheus:
        condition: service_started
    networks:
      - private

  prometheus:
    container_name: ops.prometheus
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command: --config.file=/etc/prometheus/prometheus.yml
    networks:
      - private

networks:
  default:
    name: mcp-rest-demo-net
  private:
    name: mcp-rest-private-net

volumes:
  grafana-data:
    driver: local
